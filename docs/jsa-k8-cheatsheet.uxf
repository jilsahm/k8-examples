<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<diagram program="umlet" version="14.3.0">
  <help_text>// Uncomment the following line to change the fontsize and font:
// fontsize=14
fontfamily=Monospaced //possible: SansSerif,Serif,Monospaced


//////////////////////////////////////////////////////////////////////////////////////////////
// Welcome to UMLet!
//
// Double-click on elements to add them to the diagram, or to copy them
// Edit elements by modifying the text in this panel
// Hold Ctrl to select multiple elements
// Use Ctrl+mouse to select via lasso
//
// Use +/- or Ctrl+mouse wheel to zoom
// Drag a whole relation at its central square icon
//
// Press Ctrl+C to copy the whole diagram to the system clipboard (then just paste it to, eg, Word)
// Edit the files in the "palettes" directory to create your own element palettes
//
// Select "Custom Elements &gt; New..." to create new element types
//////////////////////////////////////////////////////////////////////////////////////////////


// This text will be stored with each diagram;  use it for notes.</help_text>
  <zoom_level>5</zoom_level>
  <element>
    <id>UMLClass</id>
    <coordinates>
      <x>1450</x>
      <y>425</y>
      <w>205</w>
      <h>205</h>
    </coordinates>
    <panel_attributes>
Deployment

-
A Deployment is one of the Kubernetes objects 
that is used to manage Pods via ReplicaSets in 
a declarative way. It provides updates control 
as well as rollback functionalities. This means 
you can update or downgrade an application to 
the desired version without experiencing a user 
blackout as well as roll back to the previous 
version in case the new version is unstable or 
filled with bugs.
-
apiVersion: apps/v1
kind: Deployment
metadata:
  name: &lt;deployname&gt;
  labels:
    &lt;key&gt;: &lt;value&gt;
spec:
  replicas: &lt;n&gt;
  selector:
    matchLabels: # the labels from the Pods
      &lt;key&gt;: &lt;value&gt;
  template:
    # include metadata and spec
    # from Pod definition here
</panel_attributes>
    <additional_attributes/>
  </element>
  <element>
    <id>UMLClass</id>
    <coordinates>
      <x>890</x>
      <y>490</y>
      <w>230</w>
      <h>315</h>
    </coordinates>
    <panel_attributes>bg=#CCFFCC

Pod

-
The smalles object for running containers in K8.
There is 1 to n containers running in a Pod.
Most of the time one Pod contains one container.
-
apiVersion: v1
kind: Pod
metadata:
  name: &lt;podname&gt;
  labels:
    &lt;key&gt;: &lt;value&gt;
spec:
  serviceAccount: &lt;serviceaccountname&gt;
  containers:
  - name: &lt;containername&gt;
    image: &lt;image&gt;
    ports:
    - containerPort: &lt;port&gt;
    envFrom:
    - configMapRef:
        name: &lt;configmapname&gt;
    - secretRef:
        name: &lt;secretname&gt;
    resources:
      requests: # minimum resources 
        memory: "&lt;ram&gt;"
        cpu: "&lt;cpu&gt;"
      limits: # maximum resources
        memory: "&lt;ram&gt;"
        cpu: "&lt;cpu&gt;"
    volumeMounts:
    - name: &lt;volumename&gt;
      mountPath: &lt;where to mount in the container&gt;
  nodeSelector:
    &lt;key&gt;: &lt;value&gt; # labels of specific nodes
  volumes:
  - name: &lt;volumename&gt;
    persistentVolumeClaim:
      claimName: &lt;claimname&gt;

-..
# Link to headless serive
  subdomain: &lt;servicename&gt;
  hostname:  &lt;podhostname&gt;</panel_attributes>
    <additional_attributes/>
  </element>
  <element>
    <id>UMLClass</id>
    <coordinates>
      <x>1185</x>
      <y>485</y>
      <w>205</w>
      <h>205</h>
    </coordinates>
    <panel_attributes>
ReplicaSet

-
A ReplicaSet is a process that runs multiple 
instances of a Pod and keeps the specified 
number of Pods constant. Its purpose is to 
maintain the specified number of Pod instances 
running in a cluster at any given time to 
prevent users from losing access to their 
application when a Pod fails or is inaccessible.
-
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: &lt;replicasetname&gt;
spec:
  replicas: &lt;n&gt;
  selector:
    matchesLabels: # the labels from the Pods
      &lt;key&gt;: &lt;value&gt;
    template:
      # include metadata and spec 
      # from Pod definition here</panel_attributes>
    <additional_attributes/>
  </element>
  <element>
    <id>Relation</id>
    <coordinates>
      <x>1385</x>
      <y>565</y>
      <w>75</w>
      <h>20</h>
    </coordinates>
    <panel_attributes>lt=&lt;-
creates</panel_attributes>
    <additional_attributes>10.0;20.0;130.0;20.0</additional_attributes>
  </element>
  <element>
    <id>Relation</id>
    <coordinates>
      <x>1115</x>
      <y>575</y>
      <w>80</w>
      <h>20</h>
    </coordinates>
    <panel_attributes>lt=&lt;-
manages</panel_attributes>
    <additional_attributes>10.0;20.0;140.0;20.0</additional_attributes>
  </element>
  <element>
    <id>UMLClass</id>
    <coordinates>
      <x>875</x>
      <y>845</y>
      <w>245</w>
      <h>245</h>
    </coordinates>
    <panel_attributes>
Service

-
Ports and IPs from the view of the service:
- NodePort = The external Port on the Node the service
is reachable on
- Port = The internal Port the service is reachable from 
in the Node
- TargetPort = The Port of the Pod the service is 
delegating the trafic to
- ClusterIP = The internal IP asigned to the service 
-
FQDN Structure:
  &lt;servicename&gt;.&lt;namespace&gt;.svc.cluster.local
Pods in the sample namespace only need to use
the &lt;servicename&gt; in order to connect to it.
-..
FQND Headless Structure:
&lt;podhostname&gt;.&lt;servicename&gt;.&lt;namespace&gt;.svc.cluster.local
-
apiVersion: v1
kind: Service
metadata:
  name: &lt;servicename&gt;
spec:
  type: &lt;NodePort|ClusterIP|Loadbalancer&gt;
  ports:
  - targetPort: &lt;port of the pod&gt;
    port: &lt;port for the service&gt;
    nodePort: &lt;external port&gt; # only for type Nodeport
  selector:
    &lt;key&gt;: &lt;value&gt;
    # the labels from the target Pod

-..
# Headless-Service
  clusterIP: None</panel_attributes>
    <additional_attributes/>
  </element>
  <element>
    <id>UMLClass</id>
    <coordinates>
      <x>1030</x>
      <y>285</y>
      <w>205</w>
      <h>115</h>
    </coordinates>
    <panel_attributes>
Secret

-

-
In case a secret is mounted as a volumne each
value will be written in a file named as key
under the mounted folder.
-
apiVerions: v1
kind: Secret
metadata:
  name: &lt;secretname&gt;
data:
  &lt;key1&gt;: &lt;base64-value1&gt;
  &lt;key2&gt;: &lt;base64-value2&gt;
</panel_attributes>
    <additional_attributes/>
  </element>
  <element>
    <id>UMLClass</id>
    <coordinates>
      <x>1360</x>
      <y>285</y>
      <w>205</w>
      <h>115</h>
    </coordinates>
    <panel_attributes>
ServiceAccount

-
Used by machines in order to get access to
specific cluster operations.
-
apiVersion: v1
kind: ServiceAccount
metadata:
  name: &lt;serviceaccountname&gt;</panel_attributes>
    <additional_attributes/>
  </element>
  <element>
    <id>UMLClass</id>
    <coordinates>
      <x>800</x>
      <y>300</y>
      <w>205</w>
      <h>100</h>
    </coordinates>
    <panel_attributes>
ConfigMap

-
Outsourcing and sharing for plaintest key=value 
properties used by other objects.
-
apiVersion: v1
kind: ConfigMap
metadata:
  name: &lt;configmapname&gt;
data:
  &lt;key1&gt;: &lt;value1&gt;
  &lt;key2&gt;: &lt;value2&gt;
</panel_attributes>
    <additional_attributes/>
  </element>
  <element>
    <id>UMLClass</id>
    <coordinates>
      <x>1735</x>
      <y>40</y>
      <w>205</w>
      <h>105</h>
    </coordinates>
    <panel_attributes>
Namespace

-
Kubernetes supports multiple virtual clusters 
backed by the same physical cluster. These 
virtual clusters are called namespaces.
Namespaces are intended for use in environments 
with many users spread across multiple teams, 
or projects.
-
apiVersion: v1
kind: Namespace
metadata:
  name: &lt;namespacename&gt;</panel_attributes>
    <additional_attributes/>
  </element>
  <element>
    <id>Relation</id>
    <coordinates>
      <x>1835</x>
      <y>0</y>
      <w>110</w>
      <h>50</h>
    </coordinates>
    <panel_attributes>lt=-
can be referenced by 
any other object in its
metadata block
</panel_attributes>
    <additional_attributes>10.0;10.0;10.0;80.0</additional_attributes>
  </element>
  <element>
    <id>UMLClass</id>
    <coordinates>
      <x>1735</x>
      <y>200</y>
      <w>205</w>
      <h>190</h>
    </coordinates>
    <panel_attributes>
ResourceQuota

-
By default, containers run with unbounded 
compute resources on a Kubernetes cluster. 
With resource quotas, cluster administrators 
can restrict resource consumption and creation 
on a namespace basis. Within a namespace, a 
Pod or Container can consume as much CPU and 
memory as defined by the namespace's resource 
quota.
-
apiVersion: v1
kind: ResourceQuota
metadata:
  name: &lt;quotaname&gt;
  namespace: &lt;namespace to be limited&gt;
spec:
  hard:
    pods: &lt;n&gt;
    requests:
      cpu: "&lt;cpu&gt;"
      memory "&lt;ram&gt;"
    limits:
      cpu: "&lt;cpu&gt;"
      memory "&lt;ram&gt;" 
</panel_attributes>
    <additional_attributes/>
  </element>
  <element>
    <id>Relation</id>
    <coordinates>
      <x>1835</x>
      <y>140</y>
      <w>40</w>
      <h>70</h>
    </coordinates>
    <panel_attributes>lt=&lt;-
limits</panel_attributes>
    <additional_attributes>10.0;10.0;10.0;120.0</additional_attributes>
  </element>
  <element>
    <id>Relation</id>
    <coordinates>
      <x>895</x>
      <y>395</y>
      <w>135</w>
      <h>65</h>
    </coordinates>
    <panel_attributes>lt=-</panel_attributes>
    <additional_attributes>250.0;110.0;10.0;110.0;10.0;10.0</additional_attributes>
  </element>
  <element>
    <id>Relation</id>
    <coordinates>
      <x>1015</x>
      <y>395</y>
      <w>135</w>
      <h>65</h>
    </coordinates>
    <panel_attributes>lt=-</panel_attributes>
    <additional_attributes>250.0;10.0;250.0;110.0;10.0;110.0</additional_attributes>
  </element>
  <element>
    <id>Relation</id>
    <coordinates>
      <x>1010</x>
      <y>445</y>
      <w>60</w>
      <h>55</h>
    </coordinates>
    <panel_attributes>lt=&lt;-
injected in</panel_attributes>
    <additional_attributes>10.0;90.0;10.0;10.0</additional_attributes>
  </element>
  <element>
    <id>Relation</id>
    <coordinates>
      <x>1230</x>
      <y>350</y>
      <w>140</w>
      <h>20</h>
    </coordinates>
    <panel_attributes>lt=&lt;-
creates to 
store token</panel_attributes>
    <additional_attributes>10.0;20.0;260.0;20.0</additional_attributes>
  </element>
  <element>
    <id>UMLClass</id>
    <coordinates>
      <x>1240</x>
      <y>40</y>
      <w>205</w>
      <h>135</h>
    </coordinates>
    <panel_attributes>
RoleBinding

-

-
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: &lt;bindingname&gt;
  namespace: &lt;namespace&gt;
subjects:
- kind: ServiceAccount
  name: &lt;serviceaccountname&gt;
  namespace: default
roleRef:
  kind: &lt;Role|ClusterRole&gt;
  name: &lt;rolename|clusterrolename&gt;
  apiGroup: rbac.authorization.k8s.io</panel_attributes>
    <additional_attributes/>
  </element>
  <element>
    <id>UMLClass</id>
    <coordinates>
      <x>1475</x>
      <y>40</y>
      <w>205</w>
      <h>135</h>
    </coordinates>
    <panel_attributes>
Role

-

-
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: &lt;rolename&gt;
  namespace: &lt;namespace&gt;
rules:
- apiGroups:
  - ''
  resources: # resouces the role has access to
  - pods
  verbs: # verbs the role has access to
  - get
  - watch
  - list</panel_attributes>
    <additional_attributes/>
  </element>
  <element>
    <id>Relation</id>
    <coordinates>
      <x>1455</x>
      <y>220</y>
      <w>130</w>
      <h>75</h>
    </coordinates>
    <panel_attributes>lt=&lt;-
grant RBAC permissions to
(Role Based Access Control)</panel_attributes>
    <additional_attributes>10.0;130.0;10.0;10.0</additional_attributes>
  </element>
  <element>
    <id>Relation</id>
    <coordinates>
      <x>1455</x>
      <y>170</y>
      <w>135</w>
      <h>65</h>
    </coordinates>
    <panel_attributes>lt=-</panel_attributes>
    <additional_attributes>250.0;10.0;250.0;110.0;10.0;110.0</additional_attributes>
  </element>
  <element>
    <id>Relation</id>
    <coordinates>
      <x>1335</x>
      <y>170</y>
      <w>135</w>
      <h>65</h>
    </coordinates>
    <panel_attributes>lt=-</panel_attributes>
    <additional_attributes>250.0;110.0;10.0;110.0;10.0;10.0</additional_attributes>
  </element>
  <element>
    <id>UMLClass</id>
    <coordinates>
      <x>1965</x>
      <y>200</y>
      <w>205</w>
      <h>135</h>
    </coordinates>
    <panel_attributes>
LimitRange

-
A LimitRange is a policy to constrain resource 
allocations (to Pods or Containers) in a 
namespace.
-
apiVersion: v1
kind: LimitRange
metadata:
  name: &lt;limitrangename&gt;
spec:
  limits:
  - default:
      &lt;memory|cpu&gt;: "&lt;value&gt;"
    defaultRequest:
      &lt;memory|cpu&gt;: "&lt;value&gt;"
    type: Container
</panel_attributes>
    <additional_attributes/>
  </element>
  <element>
    <id>Relation</id>
    <coordinates>
      <x>1865</x>
      <y>140</y>
      <w>215</w>
      <h>70</h>
    </coordinates>
    <panel_attributes>lt=&lt;-
limits</panel_attributes>
    <additional_attributes>10.0;10.0;10.0;90.0;410.0;90.0;410.0;120.0</additional_attributes>
  </element>
  <element>
    <id>UMLClass</id>
    <coordinates>
      <x>1175</x>
      <y>805</y>
      <w>205</w>
      <h>230</h>
    </coordinates>
    <panel_attributes>
Job

-
A Job creates one or more Pods and will continue 
to retry execution of the Pods until a specified 
number of them successfully terminate. As pods 
successfully complete, the Job tracks the 
successful completions. When a specified number 
of successful completions is reached, the task 
(ie, Job) is complete. Deleting a Job will clean 
up the Pods it created. Suspending a Job will 
delete its active Pods until the Job is resumed 
again.
-
apiVersion: batch/v1
kind: Job
metadata:
  name: &lt;jobname&gt;
spec:
  completions: &lt;n&gt;
  parallelism: &lt;n&gt;
  template:
    spec:
      containers:
      - name: math
        image: ubuntu
        command: ['expr']
        args:
        - '3'
        - '+'
        - '5'
      restartPolicy: &lt;Never|OnFailure&gt;
</panel_attributes>
    <additional_attributes/>
  </element>
  <element>
    <id>UMLClass</id>
    <coordinates>
      <x>1450</x>
      <y>870</y>
      <w>205</w>
      <h>185</h>
    </coordinates>
    <panel_attributes>
CronJob

-
A CronJob creates Jobs on a repeating schedule.
One CronJob object is like one line of a crontab 
(cron table) file. It runs a job periodically on 
a given schedule, written in Cron format.
-
Cron Syntax
┌───────────── Minute (0 - 59)
│ ┌───────────── Second (0 - 23)
│ │ ┌───────────── Day of Month (1 - 31)
│ │ │ ┌───────────── Month (1 - 12)
│ │ │ │ ┌───────────── Day of Week (0 - 6)
│ │ │ │ │
* * * * *  
-
apiVersion: batch/v1
kind: CronJob
metadata:
  name: &lt;jobname&gt;
spec:
  scheduled: &lt;cron-string&gt;
  jobTemplate:
    # include the spec part from a Job
    # definition here
</panel_attributes>
    <additional_attributes/>
  </element>
  <element>
    <id>Relation</id>
    <coordinates>
      <x>1375</x>
      <y>935</y>
      <w>85</w>
      <h>20</h>
    </coordinates>
    <panel_attributes>lt=&lt;-
creates</panel_attributes>
    <additional_attributes>10.0;20.0;150.0;20.0</additional_attributes>
  </element>
  <element>
    <id>Relation</id>
    <coordinates>
      <x>1115</x>
      <y>710</y>
      <w>170</w>
      <h>105</h>
    </coordinates>
    <panel_attributes>lt=&lt;-
runs</panel_attributes>
    <additional_attributes>10.0;20.0;320.0;20.0;320.0;190.0</additional_attributes>
  </element>
  <element>
    <id>Relation</id>
    <coordinates>
      <x>1010</x>
      <y>800</y>
      <w>80</w>
      <h>55</h>
    </coordinates>
    <panel_attributes>lt=&lt;-
makes accessable</panel_attributes>
    <additional_attributes>10.0;10.0;10.0;90.0</additional_attributes>
  </element>
  <element>
    <id>UMLClass</id>
    <coordinates>
      <x>575</x>
      <y>910</y>
      <w>205</w>
      <h>125</h>
    </coordinates>
    <panel_attributes>
Ingress

-
Ingress exposes HTTP and HTTPS routes from 
outside the cluster to services within the 
cluster. Traffic routing is controlled by 
rules defined on the Ingress resource.
-
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: &lt;ingressname&gt;
spec:
  backend:
    serviceName: &lt;servicename&gt;
    servicePort: &lt;serviceport&gt;
</panel_attributes>
    <additional_attributes/>
  </element>
  <element>
    <id>UMLClass</id>
    <coordinates>
      <x>575</x>
      <y>770</y>
      <w>205</w>
      <h>85</h>
    </coordinates>
    <panel_attributes>lt=..

Ingress Controller

-
An Ingress controller is responsible for 
fulfilling the Ingress, usually with a load 
balancer, though it may also configure your 
edge router or additional frontends to help 
handle the traffic.
-

nginx, Traefik, and others</panel_attributes>
    <additional_attributes/>
  </element>
  <element>
    <id>Relation</id>
    <coordinates>
      <x>670</x>
      <y>850</y>
      <w>50</w>
      <h>70</h>
    </coordinates>
    <panel_attributes>lt=&lt;-
fulfills</panel_attributes>
    <additional_attributes>10.0;120.0;10.0;10.0</additional_attributes>
  </element>
  <element>
    <id>Relation</id>
    <coordinates>
      <x>775</x>
      <y>970</y>
      <w>110</w>
      <h>20</h>
    </coordinates>
    <panel_attributes>lt=&lt;-
delegates to</panel_attributes>
    <additional_attributes>200.0;20.0;10.0;20.0</additional_attributes>
  </element>
  <element>
    <id>UMLClass</id>
    <coordinates>
      <x>575</x>
      <y>1075</y>
      <w>205</w>
      <h>145</h>
    </coordinates>
    <panel_attributes>lt=..

Ingress (API - Gateway Variant)

-
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: &lt;ingressname&gt;
spec:
  rules:
  - http:
      paths:
		- path: &lt;route 1&gt;
		  backend:
		    serviceName: &lt;servicename&gt;
		    servicePort: &lt;serviceport&gt;
 		- path: &lt;route 2&gt;
		  backend:
		    serviceName: &lt;servicename&gt;
		    servicePort: &lt;serviceport&gt;
</panel_attributes>
    <additional_attributes/>
  </element>
  <element>
    <id>Relation</id>
    <coordinates>
      <x>670</x>
      <y>1030</y>
      <w>60</w>
      <h>55</h>
    </coordinates>
    <panel_attributes>lt=..
alternative</panel_attributes>
    <additional_attributes>10.0;90.0;10.0;10.0</additional_attributes>
  </element>
  <element>
    <id>UMLClass</id>
    <coordinates>
      <x>405</x>
      <y>510</y>
      <w>260</w>
      <h>215</h>
    </coordinates>
    <panel_attributes>
NetworkPolicy

-
NetworkPolicies are an application-centric construct which
allow you to specify how a pod is allowed to communicate
with various network entities over the network.
-
To use network policies, you must be using a networking 
solution which supports NetworkPolicy. Creating a 
NetworkPolicy resource without a controller that implements 
it will have no effect.
-
apiVersion: networking.k8s.io(v1
kind: NetworkPolicy
metadata:
  name: &lt;networkpolicyname&gt;
spec:
  podSelector:
    matchLabels: # the labels from the Pods to guard
      &lt;key&gt;: &lt;value&gt;
    policyTypes:
    - &lt;ingress|egress&gt;
    ingress:
    - from:
      - podSelector:
          matchLabels: # the labels from the Pods to allow
            &lt;key&gt;: &lt;value&gt;
      ports: # ports of the Pods to guard
      - protocol: &lt;TCP|UDP&gt;
        port: &lt;port&gt;</panel_attributes>
    <additional_attributes/>
  </element>
  <element>
    <id>Relation</id>
    <coordinates>
      <x>660</x>
      <y>640</y>
      <w>240</w>
      <h>20</h>
    </coordinates>
    <panel_attributes>lt=&lt;-
guards/allows</panel_attributes>
    <additional_attributes>460.0;20.0;10.0;20.0</additional_attributes>
  </element>
  <element>
    <id>UMLClass</id>
    <coordinates>
      <x>0</x>
      <y>215</y>
      <w>260</w>
      <h>185</h>
    </coordinates>
    <panel_attributes>
PersistenceVolume

-
A PersistentVolume (PV) is a piece of storage in the 
cluster that has been provisioned by an administrator 
or dynamically provisioned using Storage Classes. It 
is a resource in the cluster just like a node is a 
cluster resource. PVs are volume plugins like Volumes,
but have a lifecycle independent of any individual Pod 
that uses the PV. This API object captures the details 
of the implementation of the storage, be that NFS, iSCSI, 
or a cloud-provider-specific storage system.
-
apiVersion: v1
kind: PersistentVolume
metadata:
  name: &lt;pvname&gt;
spec:
  accessModes:
  - &lt;ReadWriteOnce|ReadOnlyMany|ReadWriteMany&gt;
  capacity:
    storage: &lt;amount&gt;
  awsElasticBlockStore: # use your PV solution here
    volumeID: &lt;id&gt;
    fsType: ext4
</panel_attributes>
    <additional_attributes/>
  </element>
  <element>
    <id>Relation</id>
    <coordinates>
      <x>660</x>
      <y>425</y>
      <w>240</w>
      <h>210</h>
    </coordinates>
    <panel_attributes>lt=-&gt;
mounts</panel_attributes>
    <additional_attributes>460.0;400.0;70.0;400.0;70.0;10.0;10.0;10.0</additional_attributes>
  </element>
  <element>
    <id>UMLClass</id>
    <coordinates>
      <x>405</x>
      <y>315</y>
      <w>260</w>
      <h>165</h>
    </coordinates>
    <panel_attributes>
PersistentVolumeClaim

-
A PersistentVolumeClaim (PVC) is a request for storage by a 
user. It is similar to a Pod. Pods consume node resources 
and PVCs consume PV resources. Pods can request specific 
levels of resources (CPU and Memory). Claims can request 
specific size and access modes (e.g., they can be mounted 
ReadWriteOnce, ReadOnlyMany or ReadWriteMany, see 
AccessModes).
-
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: &lt;claimname&gt;
spec:
  accessModes:
  - &lt;WriteOnce|ReadOnlyMany|ReadWriteMany&gt;
  resources:
    requests:
      storage: &lt;amount&gt;
  # in case a provisioned storage class is used    
  storageClassName: &lt;storageclassname&gt;</panel_attributes>
    <additional_attributes/>
  </element>
  <element>
    <id>Relation</id>
    <coordinates>
      <x>255</x>
      <y>350</y>
      <w>160</w>
      <h>25</h>
    </coordinates>
    <panel_attributes>lt=-&gt;
m1=1
m2=1
assigned to
</panel_attributes>
    <additional_attributes>300.0;20.0;10.0;20.0</additional_attributes>
  </element>
  <element>
    <id>UMLClass</id>
    <coordinates>
      <x>0</x>
      <y>430</y>
      <w>260</w>
      <h>115</h>
    </coordinates>
    <panel_attributes>
StorageClass

-
A StorageClass provides a way for administrators to 
describe the "classes" of storage they offer. Different 
classes might map to quality-of-service levels, or to 
backup policies, or to arbitrary policies determined 
by the cluster administrators.
-
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: &lt;storageclassname&gt;
provisioner: kubernetes.io/&lt;aws-ebs|gce-pd|...&gt;

</panel_attributes>
    <additional_attributes/>
  </element>
  <element>
    <id>Relation</id>
    <coordinates>
      <x>255</x>
      <y>355</y>
      <w>100</w>
      <h>170</h>
    </coordinates>
    <panel_attributes>lt=..&gt;
alternative
</panel_attributes>
    <additional_attributes>90.0;10.0;90.0;320.0;10.0;320.0</additional_attributes>
  </element>
  <element>
    <id>UMLClass</id>
    <coordinates>
      <x>1450</x>
      <y>640</y>
      <w>205</w>
      <h>210</h>
    </coordinates>
    <panel_attributes>
StatefulSet

-
Like a Deployment, a StatefulSet manages Pods 
that are based on an identical container spec. 
Unlike a Deployment, a StatefulSet maintains a 
sticky identity for each of their Pods. These 
pods are created from the same spec, but are 
not interchangeable: each has a persistent 
identifier that it maintains across any 
rescheduling.
-
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: &lt;statefulsetname&gt;
  labels:
    &lt;key&gt;: &lt;value&gt;
spec:
  replicas: &lt;n&gt;
  selector:
    matchLabels: # the labels from the Pods
      &lt;key&gt;: &lt;value&gt;
  serviceName: &lt;servicename&gt;
  template:
    # include metadata and spec
    # from Pod definition here
  volumeClaimTemplate:
    # include the PVC definition here. Each Pod
    # will then get its own PVC
</panel_attributes>
    <additional_attributes/>
  </element>
  <element>
    <id>Relation</id>
    <coordinates>
      <x>1425</x>
      <y>570</y>
      <w>35</w>
      <h>205</h>
    </coordinates>
    <panel_attributes>lt=-
</panel_attributes>
    <additional_attributes>10.0;10.0;10.0;390.0;50.0;390.0</additional_attributes>
  </element>
  <element>
    <id>UMLClass</id>
    <coordinates>
      <x>1735</x>
      <y>550</y>
      <w>205</w>
      <h>180</h>
    </coordinates>
    <panel_attributes>
Deployment

-
The Horizontal Pod Autoscaler automatically scales 
the number of Pods in a replication controller, 
deployment, replica set or stateful set based on 
observed CPU utilization (or, with custom metrics 
support, on some other application-provided
metrics). Note that Horizontal Pod Autoscaling 
does not apply to objects that can't be scaled, 
for example, DaemonSets.
-
apiVersion: autoscaling/v1
kind: HorizontalPodAutoscaler
metadata:
  name: &lt;scalename&gt;
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: &lt;deploymentname&gt;
  minReplicas: 1
  maxReplicas: 10
  targetCPUUtilizationPercentage: 50
</panel_attributes>
    <additional_attributes/>
  </element>
  <element>
    <id>Relation</id>
    <coordinates>
      <x>1650</x>
      <y>590</y>
      <w>95</w>
      <h>20</h>
    </coordinates>
    <panel_attributes>lt=&lt;-
autoscales</panel_attributes>
    <additional_attributes>10.0;20.0;170.0;20.0</additional_attributes>
  </element>
  <element>
    <id>Relation</id>
    <coordinates>
      <x>1650</x>
      <y>595</y>
      <w>35</w>
      <h>185</h>
    </coordinates>
    <panel_attributes>lt=&lt;-</panel_attributes>
    <additional_attributes>10.0;350.0;50.0;350.0;50.0;10.0</additional_attributes>
  </element>
</diagram>
